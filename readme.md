# CS 584-A Natural Language Processing
Department of Computer Science, Fall 2025
Instructor: Ping Wang
Course Schedule: Monday, 3:00 PM-5:30 PM, 2025 Fall
Course Location: Burchard 103
Virtual Office Hours: Tuesday, 1-2PM. Zoom:

## COURSE DESCRIPTION
Natural language processing (NLP) is one of the most important technologies referring to automatic computational processing of human languages. 
This includes algorithms that take human-produced text as input or produce text as output. People communicate almost everything
in language: emails, phone calls, language translation, web searches, reports, books, social media, etc. 
Human language is symbolic in nature and also highly ambiguous and variable.
Comprehending human language is a crucial and challenging part of artificial intelligence. 
There are a large variety of underlying tasks and machine learning models behind NLP applications.
Recently, deep learning approaches have been studied and achieved high performance in many NLP tasks. 
The course provides an introduction to machine learning and deep learning research applied to NLP. 
We will cover topics including word vector representations, neural networks, recurrent neural networks, convolutional neural networks, 
seq2seq models, attention-based models, transformers, pre-training, fine-tuning, prompting, and many other advanced NLP techniques and applications.
Stevens Course Syllabus Template: January 17, 2023 2
Prerequisites:
This course is fast-paced and covers a lot of ground, so it is important that you have a solid foundation on both the theoretical and empirical fronts. 
You should have background in python programming, probability theory, linear algebra, calculus, and foundations of machine learning.

## STUDENT LEARNING OUTCOMES
After successful completion of this course, students will be able to
• Understand the neural networks models and backpropagation optimization. Implement neural network models in TensorFlow.
• Apply word2vec models in real-world text corpora.
• Understand convolutional neural networks and their application in NLP.
• Understand sequence to sequence models and attention in NLP deep neural networks.
• Understand dependency parsing, recurrent neural networks and their application in NLP.
• Implement gradient descent (GD) and stochastic gradient descent (SGD) techniques for learning problems and understand the theory behind them.
• Understand the recent advances in NLP, such as transformers, pre-training, fine-tuning, prompting, etc.